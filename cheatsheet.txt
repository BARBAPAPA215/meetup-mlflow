# Motivation
- PoC to prod 
- Simple
- Remote execution
- Cloud integration
- Modular

# Check of audience
- No. experienced python users?
- No. experience with Databricks?
- No. experience with Azure?
- No. experience with Databricks?

# Add git config
git config --global user.email "user@test.com"
git config --global username "thomasht86"
git config --global credential.helper store


# Add git SSH
https://help.github.com/en/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent

# Start mlflow ui
Remember that location matters

# Run training locally (in same repo)
mlflow run .

# Run MLproject from git
mlflow run git@github.com:thomasht86/meetup-mlflow.git

# Deploy prediction server locally
mlflow models serve -m /home/thomas/meetup-mlflow/mlruns/0/54a2c454368047048a6d965bc5b3d350/artifacts/models -p 1234

# Make predictions locally
curl -X POST -H "Content-Type:application/json; format=pandas-split" --data '{"columns"["description"],"data":[["medicine engineering"],["test medicine medical surgery"]]}' http://127.0.0.1:1234/invocations

# Create azure account
Get subscription_id

# Deploy model to azureml
See notebook

## Training 
- Demonstrate training+tracking on local computer
- (Demonstrate training+tracking on Databricks)

## Comparing
- MlFlow UI
- Access results programmatically

## Serving
- Serving from local computer
- Deploying to Azure